#this is the code for the project in simple markdown
streaming framework for analyzing tweets for a given topic

We will:

1. Keep track and visualize (with a wordcloud) common terms associated
   with the topic.
2. Classify and visualize the polarity (positive/negative/neutral) of
   tweets and visual common words in each class.
3. Keep track of the proportion of positive tweets as time goes on.
4. Keep track of median values for number of unique words in the arriving tweets 
   over a given time period.
5. Visualize and report the rate of tweets in a given time frame.

```{r install.packages, eval = FALSE}

*install the packages*

install.packages("ggplot2")  
install.packages("RStorm")  
install.packages("twitteR")  
install.packages("wordcloud")  
install.packages("dplyr")  
install.packages("tidyr")  
install.packages("RColorBrewer") 
```

```{r install.sentiment, eval = FALSE}
install.packages("tm")
#### choose either of three options
#install.packages("http://www.omegahat.org/Rstem/Rstem_0.4-1.tar.gz",
 #              repo = NULL, type = "source")


install.packages("Rstem")


install.packages("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz",
	             repo = NULL, type = "source")
```


Once all the packages are install, we can load the libraries.

```{r}
library(ggplot2)  
library(RStorm)  
library(twitteR)  
library(sentiment)  
library(wordcloud)  
library(dplyr)  
library(tidyr)  
library(RColorBrewer) 
library(Rstem)
```

## Getting Tweets
### Authorizing `twitteR`
In order to search for tweets with `twitteR`, you need to have a valid Twitter account to obtain authorization credentials.
To start, you will need to enter your consumer key and secret and access token and secret, then call
`setup_twitter_oath()` with these strings.  To get your access key,
secret, and tokens: 

1. Have a valid Twitter Account
2. Go to [https://apps.twitter.com/](https://apps.twitter.com/) and sign in
3. Click `Create New App` if you don't already have one
4. You can fill in dummy values for Name, Description, and Website
5. Once you're in your App, click on `Keys and Access Tokens`
6. Your consumer keys should already exist.  Click `Create my access token` for the access token and secret.
7. Copy and paste the key, token, and secrets into the following code:


```{r Authorize, eval = FALSE}
consumer.key <- 'B85sJbS9MqCun9J08izFThSAm'
consumer.secret <- 'QlKjVlbT7iroL4bkfvrBcv5XBTrkwce9jceH0RgHk16cWhif42'
access.token <- '2983706704-mWmUWbqpYClDYCy9HLvQeQLZ351Pylt18EBf7PM'
access.secret <- '3Rx7YUxQ1OjDLfEuDDha1Meuo0MkC2PDvvmwjh6hH0CK9'
setup_twitter_oauth(consumer.key, consumer.secret,
                    access.token, access.secret)
```

### Searching for Tweets
`twitteR` can search for recent tweets using Twitter's REST APIs.

```{r get.tweets, cache = TRUE}
tweet.list <- searchTwitter(searchString = "bigdata", 
                            n = 1500, 
                            lang = "en")
tweet.df <- twListToDF(tweet.list)
colnames(tweet.df)
dim(tweet.df)
```

> `searchTwitter()` will put the most recent tweets at the top of the `data.frame`, so we'll want to reverse it to simulate tweets arriving in realtime.

```{r reorder.df}
tweet.df <- tweet.df[order(tweet.df$created),]
```

## Setting up the Topology
Now that we have a `data.frame` of tweets, we can use these to simulate an `RStorm` topology.  

Bolt | Uses
-----|-------------
`track.rate()` | Calculate and track tweets per minute over time
`get.text()` | Extract the texts from tweet
`clean.text()` | Clean special characters, links, punctuation, etc.
`strip.stopwords()` | Clean conjunctions, prepositions, etc. (handy)
`get.word.counts()` | Create and update word counts (as it arrives)
`get.polarity()` | Classify polarity of a tweet (negative, positive , neutral)
`track.polarity()` | Track percentage of positive/negative/neutral tweets over time
`store.words.polarity()` | Store words for each polarity level
`unique.median` | collect and send the median value of unique words(collectively for all the tweets) over a given time.

We will need the following hashes and trackers to calculate and track our results:

`data.frame` | Role | Description
-------------|------|---------------
`bigdata.df` | Spout | Table to simulate tweets 
`word.counts.df` | Hash | Stores word frequencies
`t.stamp.df` | Hash | Store unique time stamps
`tpm.df` | Tracker | Track tweets per minute over time
`prop.df` | Tracker | Track percentage of each polarity over time
`polarity.df` | Hash | Store word counts per polarity (term document matrix)
`polar.words.df` | Hash | Keep track of words associated with each polarity
`uniques.df` | Tracker | Track the given values for our requirements.

### The Spout
The `data.frame` `tweet.df` will be used to simulate tweets arriving in realtime.  
In `RStorm`, we start the topology by specifying the spout.

```{r start.topo}
topo <- Topology(tweet.df)
```

```{r stringAsFactors_FALSE}
options(stringsAsFactors = FALSE)
```

#### Bolt 1: `track.rate()`
`track.rate()` calculates the tweets per minute (tpm) for a new tweet.  

Once we've tracked the tpm rate, we simply close the function, since no bolts are downstream.  Finally, we create a bolt from the function which listens to the spout and add it to the topology.

```{r track.rate}
track.rate <- function(tuple, ...){
    t.stamp <- tuple$created
    ## track current time stamp
    t.stamp.df <- GetHash("t.stamp.df")
    if(!is.data.frame(t.stamp.df)) t.stamp.df <- data.frame()
    t.stamp.df <- rbind(t.stamp.df, 
                        data.frame(t.stamp = t.stamp))
    SetHash("t.stamp.df", t.stamp.df)
    
    ## get all time stamps and find when a minute ago was
    t.stamp.past <- t.stamp.df$t.stamp
    last.min <- t.stamp - 60
    ## get tpm if we're a minute into the stream
    if(last.min >= min(t.stamp.past)){
        in.last.min <- (t.stamp.past >= last.min) & (t.stamp.past <= t.stamp)
        tpm <- length(t.stamp.past[in.last.min])
    } else {
        tpm <- length(t.stamp.past)
    }
    TrackRow("tpm.df", data.frame(tpm = tpm, t.stamp = t.stamp))
}
topo <- AddBolt(topo, Bolt(track.rate, listen = 0, boltID = 1))
```

#### Bolt 2: `get.text()`

This bolt doesn't depend on `track.rate()`, so it also listens to the spout.

```{r get.text}
get.text <- function(tuple, ...){
    Emit(Tuple(data.frame(text = tuple$text,
                          t.stamp = tuple$created)), ...)
}
topo <- AddBolt(topo, Bolt(get.text, listen = 0, boltID = 2))
```

#### Bolt 3: `clean.text()`
this takes raw text from `get.text()`, converts it to a more flexible text encoding, forces it to lower case, and strips it of hyperlinks, possesives, special characters, punctuation, and extra whitespace.

After the text is clean, we emit the clean text and continue passing the time stamp down the stream.

```{r clean.text}
clean.text <- function(tuple, ...){
    text.clean <- tuple$text %>%
        ## convert to UTF-8
        iconv(to = "UTF-8") %>%
        ## strip URLs
        gsub("\\bhttps*://.+\\b", "", .) %>% 
        ## force lower case
        tolower %>%
        ## get rid of possessives
        gsub("'s\\b", "", .) %>%
        ## strip html special characters
        gsub("&.*;", "", .) %>%
        ## strip punctuation
        removePunctuation %>%
        ## make all whitespace into spaces
        gsub("[[:space:]]+", " ", .)
        
    Emit(Tuple(data.frame(text = text.clean, t.stamp = tuple$t.stamp)), ...)
}
topo <- AddBolt(topo, Bolt(clean.text, listen = 2, boltID = 3))
```

#### Bolt 4: `strip.stopwords()`

```{r strip.stopwords}
strip.stopwords <- function(tuple, ...){
    text.content <- removeWords(tuple$text,
                                removePunctuation(stopwords("SMART"))) %>%
        gsub("[[:space:]]+", " ", .)
    if(text.content != " " && !is.na(text.content)){
        Emit(Tuple(data.frame(text = text.content,
                              t.stamp = tuple$t.stamp)), ...)
    }
}
topo <- AddBolt(topo, Bolt(strip.stopwords, listen = 3, boltID = 4))
```

#### Bolt 5: `get.word.counts()`
counts the number of times each word appears.  
Since this is the end of a stream branch, we don't need to emit anything.

```{r get.word.counts}
get.word.counts <- function(tuple, ...){
    words <- unlist(strsplit(tuple$text, " "))
    words.df <- GetHash("word.counts.df")
    if(!is.data.frame(words.df)) words.df <- data.frame()
    sapply(words, function(word){
               if(word %in% words.df$word){
                   words.df[word == words.df$word,]$count <<-
                       words.df[word == words.df$word,]$count + 1
               } else{
                   words.df <<- rbind(words.df,
                                     data.frame(word = word, count = 1))
               }
           }, USE.NAMES = FALSE)
    SetHash("word.counts.df", words.df)
}
topo <- AddBolt(topo, Bolt(get.word.counts, listen = 4, boltID = 5))
```

#### Bolt 6:
Bolt 6, `get.polarity()` listens to `strip.stopwords()`.  The purpose of this bolt is to classify the polarity of a given tweet.  The `classify_polarity()` function implements a Naive-Bayes classifier which uses the polarity of the words in the tweet to predict the polarity of the entire tweet.  For example,

```{r sentiment.sample}
##download the archived sentiment package and install and load it.
download.file("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz", "sentiment.tar.gz")
install.packages("sentiment.tar.gz", repos=NULL, type="source")
library(sentiment)
classify_polarity("I love data")
classify_polarity("I hate data")
```

For the given tweet, we classify the polarity and pass the text, time stamp, and polarity down the stream.

```{r get.polarity}
get.polarity <- function(tuple, ...){
    polarity <- classify_polarity(tuple$text)[,4]
    Emit(Tuple(data.frame(text = tuple$text,
                          t.stamp = tuple$t.stamp,
                          polarity = polarity)), ...)
}
topo <- AddBolt(topo, Bolt(get.polarity, listen = 4, boltID = 6))
```

#### Bolt 7: `track.polarity()`

`track.polarity()` takes the polarity and time stamp from `get.polarity()` and uses them to keep track of the cumulative percentage of tweets of each polarity over time.

We start by getting the `polarity.df` hash, or creating it if it
hasn't been created yet.  After getting the `data.frame`, we increment
the polarity count corresponding to our tweet and the total number
tweets and update the hash of the counts.

Using these, we find the cumulative proportion of tweets with each
polarity and track the fractions.


```{r track.polarity}
track.polarity <- function(tuple, ...){
    polarity <- tuple$polarity
    
    polarity.df <- GetHash("polarity.df")
    if(!is.data.frame(polarity.df)){
        polarity.df <- data.frame(positive = 0,
                                  neutral = 0,
                                  negative = 0,
                                  n = 0)
    }
    polarity.df[1, c(polarity, "n")] <- polarity.df[1, c(polarity, "n")] + 1
    SetHash("polarity.df", polarity.df)

    prop.df <- data.frame(cbind(polarity.df[1, 1:3]/polarity.df[1, "n"],
                                t.stamp = tuple$t.stamp))
    TrackRow("prop.df", prop.df)
}
topo <- AddBolt(topo, Bolt(track.polarity, listen = 6, boltID = 7))
```


#### Bolt 8: `store.words.polarity()`
The function works similar to `get.word.counts()`, with one extra
caveat. Instead of having an $n \times 1$ data.frame, we have an `n
\times 3` data.frame, with a column for each polarity. In natural
language processing, we would call this a Term Document Matrix (TDM).
The TDM has a column for each "document" we are examining, in this
case polarity classes.  Then, we have a row for every unique word,
with the values representing the counts within each document.

If we've encountered the word before, we increment the word count in
the column corresponding to the tweet's polarity.

If it is the first time we are seeing a word, we first create a new
row of zeros, add a count of 1 in the correct polarity column,
then bind this row to our existing term document matrix.

```{r store.words.polarity}
store.words.polarity <- function(tuple, ...){
    polar.words.df <- GetHash("polar.words.df")
    if(!is.data.frame(polar.words.df)) polar.words.df <- data.frame()
    
    words <- unlist(strsplit(tuple$text, " "))
    polarity <- tuple$polarity

    sapply(words, function(word){
               if(word %in% rownames(polar.words.df)){
                   polar.words.df[word, polarity] <<-
                       polar.words.df[word, polarity] + 1
               } else {
                   n <- nrow(polar.words.df)
                   this.row <- data.frame(positive = 0, neutral = 0,
                                          negative = 0)
                   this.row[1, polarity] <- 1
                   polar.words.df <<- rbind(polar.words.df,
                                            na.omit(this.row))
                   rownames(polar.words.df)[n + 1] <<- word
               }
          })
      SetHash("polar.words.df", polar.words.df)
}
topo <- AddBolt(topo, Bolt(store.words.polarity, listen = 6, boltID = 8))
```

## Lets run the median number for unique words over given time. Tracker is `uniques.df`


```{r unique.median}

unique.median <- function(tuple, ...){
  unique.words <- median(length(unique(unlist(strsplit(tuple$text, " ")))))
  #Emit(Tuple(data.frame(uniques = unique.words, t.stamp = tuple$t.stamp)), ...)
  TrackRow("uniques.df", data.frame(uniques = unique.words, t.stamp = tuple$t.stamp))
}
topo <- AddBolt(topo, Bolt(unique.median, listen = 4, boltID = 9))
```




## Running the Topology
Now that our bolts are created and added to the topology, we can run the simulation.  Note that, depending on how many tweets you pulled and your processor speed, this may take several minutes.

```{r run.topo, cache = TRUE}
topo
system.time(result <- RStorm(topo))
result
```

## Analyzing the Results
We can get our results by extracting the hashes and trackers from the `result` object.

### Word Frequencies: Word Clouds
The `wordcloud()` function draws a word cloud given a `vector` of words and a `vector` of frequencies, which make up the columns of the hashed `data.frame` `word.counts.df`.
```{r word.cloud, cache = TRUE}
color.vec <- c("black", rep("red", 5))
word.df <- GetHash("word.counts.df", result)
words <- word.df$word
counts <- word.df$count
wordcloud(words, counts, scale = c(3, 1), max.words = 100, min.freq = 5, 
          colors = color.vec)
```

### Polarity: Comparison Cloud
We can extract the word lists from `polar.words.df` to build the comparison cloud.

```{r comparison.cloud, cache = TRUE}
polar.words.df <- na.omit(GetHash("polar.words.df", result))
comparison.cloud(polar.words.df, min.freq = 10, scale = c(3, 1), 
                 colors = c("cornflowerblue", "black", "red"),
                 random.order = FALSE)
```

### Polarity over Time
The `prop.df` tracker is used to make a timeplot of the percentages of each polarity over time.  To plot the percentages over time in `ggplot2`, we first need to convert the data from a wide format to a long format.

```{r prop.plot}
prop.df <- GetTrack("prop.df", result)
prop.df.long <- prop.df %>% gather(Polarity, Proportion, -t.stamp)
ggplot(prop.df.long, aes(x = t.stamp, y = Proportion, color = Polarity)) +
   geom_line() + theme(legend.position = "top") + 
   scale_color_manual(values = c("cornflowerblue", "black", "red"))
```

### Tweet Rate over Time

```{r tpm.plot}
tpm.df <- GetTrack("tpm.df", result)
ggplot(tpm.df, aes(x = t.stamp, y = tpm)) + 
    geom_line() + geom_smooth(se = FALSE, linetype = "dashed", size = 1.5)
```

### median unique words over time plot


```{r median.plot}
uniques.df <- GetTrack("uniques.df", result)
ggplot(uniques.df, aes(x= t.stamp, y=uniques))+ geom_point(shape=5, color="darkgreen")+
  geom_smooth(method="lm") 
```

```{r stringsAsFactors_TRUE}
options(stringsAsFactors = TRUE)
```
